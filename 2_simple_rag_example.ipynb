{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76e18c11-69e2-46ae-8d6d-29f407e285f9",
      "metadata": {
        "id": "76e18c11-69e2-46ae-8d6d-29f407e285f9"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmalbornoz/SimpleRAG/blob/main/2_simple_rag_example.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1d5c67-02f3-4aaa-9714-f069ab1ba774",
      "metadata": {
        "id": "4f1d5c67-02f3-4aaa-9714-f069ab1ba774"
      },
      "source": [
        "# Simple RAG workflow\n",
        "## Dr José M Albornoz\n",
        "### April 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "metadata": {
        "id": "Hbi5yWlffhj1"
      },
      "id": "Hbi5yWlffhj1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8418f331-7311-4f99-8237-9222c5377495",
      "metadata": {
        "id": "8418f331-7311-4f99-8237-9222c5377495"
      },
      "source": [
        "# 0.- Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "daa138a3-5bdd-473e-a8ed-2bbf11928f0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa138a3-5bdd-473e-a8ed-2bbf11928f0a",
        "outputId": "546575a1-145d-42f3-c5c8-a5fd1b79950a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "!pip install torch==2.2.0 --no-warn-script-location > /dev/null\n",
        "!pip install langchain==0.0.335 --no-warn-script-location > /dev/null\n",
        "!pip install pygpt4all==1.1.0 --no-warn-script-location > /dev/null\n",
        "!pip install gpt4all==1.0.12 --no-warn-script-location > /dev/null\n",
        "!pip install transformers==4.35.1 --no-warn-script-location > /dev/null\n",
        "!pip install datasets==2.14.6 --no-warn-script-location > /dev/null\n",
        "!pip install tiktoken==0.4.0 --no-warn-script-location > /dev/null\n",
        "!pip install chromadb==0.4.15 --no-warn-script-location > /dev/null\n",
        "!pip install sentence_transformers==2.2.2 --no-warn-script-location > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ae61ce-a94c-4aaa-82f1-8a0083be9a49",
      "metadata": {
        "id": "f1ae61ce-a94c-4aaa-82f1-8a0083be9a49"
      },
      "source": [
        "# 1.- Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d2883854-37fc-40c5-9842-47d708c5eaf1",
      "metadata": {
        "id": "d2883854-37fc-40c5-9842-47d708c5eaf1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import contextlib\n",
        "import pandas as pd\n",
        "import time\n",
        "import io\n",
        "\n",
        "from tqdm import tqdm\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.- Define model URL and model folder"
      ],
      "metadata": {
        "id": "vD7ertNdwK_A"
      },
      "id": "vD7ertNdwK_A"
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Gam2GgwQmd",
        "outputId": "c1b4be12-3f6e-4064-ae42-4a9fe5dd4742"
      },
      "id": "R6Gam2GgwQmd",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/resolve/main/ggml-model-gpt4all-falcon-q4_0.bin'"
      ],
      "metadata": {
        "id": "vZY10kt6XAQD"
      },
      "id": "vZY10kt6XAQD",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bd9190d-63dd-4052-901c-aaf338131db7",
      "metadata": {
        "id": "7bd9190d-63dd-4052-901c-aaf338131db7"
      },
      "source": [
        "# 3.- Define RagBot class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0df4e4be-d7af-4736-841e-6ac1d08bf53a",
      "metadata": {
        "id": "0df4e4be-d7af-4736-841e-6ac1d08bf53a"
      },
      "outputs": [],
      "source": [
        "class RAGBot:\n",
        "    \"\"\"\n",
        "    A class to handle model downloading, dataset management, model loading, vector database\n",
        "    creation, retrieval mechanisms, and inference for a response generation bot.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    model_path : str\n",
        "        The file path where the model is stored.\n",
        "    data_path : str\n",
        "        The file path where the dataset is stored.\n",
        "    user_input : str\n",
        "        The input provided by the user for generating a response.\n",
        "    model : str\n",
        "        The name of the model being used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the RAGBot with default values for model path, data path,\n",
        "        user input, and model.\n",
        "        \"\"\"\n",
        "        self.model_path = \"\"\n",
        "        self.data_path = \"\"\n",
        "        self.user_input = \"\"\n",
        "        self.model = \"\"\n",
        "\n",
        "    def get_model(self, model, chunk_size: int = 10000):\n",
        "        \"\"\"\n",
        "        Downloads the specified model to the model path. Supports downloading of large\n",
        "        models in chunks.\n",
        "\n",
        "        Additional download tooling is reserved for users to add their own models. Currently hardcoded to load Falcon from\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : str\n",
        "            The name of the model to be downloaded.\n",
        "        chunk_size : int, optional\n",
        "            The size of each chunk of data to download at a time, by default 10000.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model = model\n",
        "        self.model_path = \"/content/models/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
        "\n",
        "        if not os.path.isfile(self.model_path):\n",
        "\n",
        "            print('Downloading ggml model')\n",
        "\n",
        "            # send a GET request to the URL to download the file. Stream since it's large\n",
        "            response = requests.get(url, stream=True)\n",
        "\n",
        "            # open the file in binary mode and write the contents of the response to it in chunks\n",
        "            # This is a large file, so be prepared to wait.\n",
        "            with open(self.model_path, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=10000)):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "        else:\n",
        "            print('model already exists in path.')\n",
        "\n",
        "    def download_dataset(self, dataset):\n",
        "        \"\"\"\n",
        "        Downloads the specified dataset and saves it to the data path.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : str\n",
        "            The name of the dataset to be downloaded.\n",
        "        \"\"\"\n",
        "        self.data_path = dataset + '_dialogues.txt'\n",
        "\n",
        "        if not os.path.isfile(self.data_path):\n",
        "\n",
        "            datasets = {\"robot maintenance\": \"FunDialogues/customer-service-robot-support\",\n",
        "                        \"basketball coach\": \"FunDialogues/sports-basketball-coach\",\n",
        "                        \"physics professor\": \"FunDialogues/academia-physics-office-hours\",\n",
        "                        \"grocery cashier\" : \"FunDialogues/customer-service-grocery-cashier\"}\n",
        "\n",
        "            # Download the dialogue from hugging face\n",
        "            print('downloading dialog dataset')\n",
        "            dataset = load_dataset(f\"{datasets[dataset]}\")\n",
        "            # Convert the dataset to a pandas dataframe\n",
        "            dialogues = dataset['train']\n",
        "            df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n",
        "            # Print the first 5 rows of the dataframe\n",
        "            df.head()\n",
        "            # only keep the dialogue column\n",
        "            dialog_df = df['dialogue']\n",
        "\n",
        "            # save the data to txt file\n",
        "            dialog_df.to_csv(self.data_path, sep=' ', index=False)\n",
        "        else:\n",
        "            print('data already exists in path.')\n",
        "\n",
        "    def load_model(self, n_threads, max_tokens, repeat_penalty, n_batch, top_k, temp):\n",
        "        \"\"\"\n",
        "        Loads the model with specified parameters for parallel processing.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_threads : int\n",
        "            The number of threads for parallel processing.\n",
        "        max_tokens : int\n",
        "            The maximum number of tokens for model prediction.\n",
        "        repeat_penalty : float\n",
        "            The penalty for repeated tokens in generation.\n",
        "        n_batch : int\n",
        "            The number of batches for processing.\n",
        "        top_k : int\n",
        "            The number of top k tokens to be considered in sampling.\n",
        "        \"\"\"\n",
        "        # Callbacks support token-wise streaming\n",
        "        callbacks = [StreamingStdOutCallbackHandler()]\n",
        "\n",
        "        # Verbose is required to pass to the callback manager\n",
        "        print('Loading model...')\n",
        "\n",
        "        self.llm = GPT4All(model=self.model_path, callbacks=callbacks, verbose=False,\n",
        "                           n_threads=n_threads, n_predict=max_tokens, repeat_penalty=repeat_penalty,\n",
        "                           n_batch=n_batch, top_k=top_k, temp=temp)\n",
        "\n",
        "    def build_vectordb(self, chunk_size, overlap):\n",
        "        \"\"\"\n",
        "        Builds a vector database from the dataset for retrieval purposes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        chunk_size : int\n",
        "            The size of text chunks for vectorization.\n",
        "        overlap : int\n",
        "            The overlap size between chunks.\n",
        "        \"\"\"\n",
        "        loader = TextLoader(self.data_path)\n",
        "\n",
        "        # Text Splitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "\n",
        "        # Embed the document and store into chroma DB\n",
        "        self.index = VectorstoreIndexCreator(embedding= HuggingFaceEmbeddings(), text_splitter=text_splitter).from_loaders([loader])\n",
        "\n",
        "    def retrieval_mechanism(self, user_input, top_k=1, context_verbosity = False, rag_off= False):\n",
        "        \"\"\"\n",
        "        Retrieves relevant document snippets based on the user's query.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        user_input : str\n",
        "            The user's input or query.\n",
        "        top_k : int, optional\n",
        "            The number of top results to return, by default 1.\n",
        "        context_verbosity : bool, optional\n",
        "            If True, additional context information is printed, by default False.\n",
        "        rag_off : bool, optional\n",
        "            If True, disables the retrieval-augmented generation, by default False.\n",
        "        \"\"\"\n",
        "\n",
        "        self.user_input = user_input\n",
        "        self.context_verbosity = context_verbosity\n",
        "\n",
        "        # perform a similarity search and retrieve the context from our documents\n",
        "        results = self.index.vectorstore.similarity_search(self.user_input, k=top_k)\n",
        "\n",
        "        # join all context information into one string\n",
        "        context = \"\\n\".join([document.page_content for document in results])\n",
        "        if self.context_verbosity:\n",
        "            print(f\"Retrieving information related to your question...\")\n",
        "            print(f\"Found this content which is most similar to your question: {context}\")\n",
        "\n",
        "        if rag_off:\n",
        "            template = \"\"\"Question: {question}\n",
        "            Answer: This is the response: \"\"\"\n",
        "            self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "        else:\n",
        "            template = \"\"\" Don't just repeat the following context, use it in combination with your knowledge to improve your answer to the question:{context}\n",
        "\n",
        "            Question: {question}\n",
        "            \"\"\"\n",
        "            self.prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
        "\n",
        "\n",
        "    def inference(self):\n",
        "        \"\"\"\n",
        "        Performs inference to generate a response based on the user's query.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            The generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.context_verbosity:\n",
        "            print(f\"Your Query: {self.prompt}\")\n",
        "\n",
        "        llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
        "\n",
        "        print(\"Processing the information with gpt4all...\\n\")\n",
        "        response = llm_chain.run(self.user_input)\n",
        "\n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbddb7a8-bb22-4764-b6e7-1c909cfeee4d",
      "metadata": {
        "id": "cbddb7a8-bb22-4764-b6e7-1c909cfeee4d"
      },
      "source": [
        "# 3.- Implement RAGBot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = RAGBot()"
      ],
      "metadata": {
        "id": "nWjYtZFOpbu-"
      },
      "id": "nWjYtZFOpbu-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_threads = 64"
      ],
      "metadata": {
        "id": "H7PS1Z6aptY2"
      },
      "id": "H7PS1Z6aptY2",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 50"
      ],
      "metadata": {
        "id": "OXK5efDGpy3Z"
      },
      "id": "OXK5efDGpy3Z",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repeat_penalty = 1.5"
      ],
      "metadata": {
        "id": "heyZjStgp4aW"
      },
      "id": "heyZjStgp4aW",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_batch = n_threads"
      ],
      "metadata": {
        "id": "a9b3cgX1yhuH"
      },
      "id": "a9b3cgX1yhuH",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 2"
      ],
      "metadata": {
        "id": "gLhM31lLyl8A"
      },
      "id": "gLhM31lLyl8A",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = 0.7"
      ],
      "metadata": {
        "id": "wBtVPpqxyryc"
      },
      "id": "wBtVPpqxyryc",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.get_model(model = 'Falcon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4dgbgeu5E2J",
        "outputId": "89c34ca8-a2ec-4db6-fc03-bbdf1fbc9aee"
      },
      "id": "I4dgbgeu5E2J",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model already exists in path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot.load_model(n_threads=n_threads, max_tokens=max_tokens, repeat_penalty=repeat_penalty, n_batch=n_threads, top_k=top_k, temp=temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKvHOKGjpfMY",
        "outputId": "bc8db07e-e9de-4e38-c4af-8a12a24ff4f1"
      },
      "id": "BKvHOKGjpfMY",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Found model file at  /content/models/ggml-model-gpt4all-falcon-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot.download_dataset(dataset = 'grocery cashier')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDom2aN75xxO",
        "outputId": "8484f293-e34b-45cc-b6e8-7b0fac1c10bd"
      },
      "id": "qDom2aN75xxO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data already exists in path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot.build_vectordb(chunk_size = 500, overlap = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgF92xXX6PmG",
        "outputId": "5f39dbba-23a3-4029-9be1-848642dbe8bd"
      },
      "id": "BgF92xXX6PmG",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"do you sell oat milk?\""
      ],
      "metadata": {
        "id": "jLMkBfd78Iyv"
      },
      "id": "jLMkBfd78Iyv",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.retrieval_mechanism(user_input = query, rag_off = False)"
      ],
      "metadata": {
        "id": "1rj7xRCA6q1H"
      },
      "id": "1rj7xRCA6q1H",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = bot.inference()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNcYa0t-8WJB",
        "outputId": "ca634ec4-0e27-4021-a820-12632d7589f3"
      },
      "id": "yNcYa0t-8WJB",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing the information with gpt4all...\n",
            "\n",
            "\n",
            "Answer: No, we only offer almond milk at our store."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "psutil.virtual_memory()"
      ],
      "metadata": {
        "id": "rlTQ_H5UPq-x",
        "outputId": "23d33710-12d4-40ce-b277-cb89704d263a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rlTQ_H5UPq-x",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "svmem(total=13609451520, available=7155441664, percent=47.4, used=6115717120, free=4698497024, active=1174745088, inactive=7248003072, buffers=350019584, cached=2445217792, shared=5074944, slab=360239104)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dq9c7U3UdcYd"
      },
      "id": "Dq9c7U3UdcYd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}